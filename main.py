# main.py

import time
import math
import csv
import json
from config import BASE_URL, MAX_PAGES_TO_SCRAPE, AUTO_DETECT_PAGES, SLEEP_DELAY_SECONDS, EXPORT_FORMAT, EXPORT_FILENAME
from scraper import get_page_data


def save_results(apartments, file_format, filename):
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–æ–±—Ä–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ —Ñ–∞–π–ª —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞ (csv –∏–ª–∏ json).
    """
    if not apartments:
        print("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è.")
        return

    full_filename = f"{filename}.{file_format}"
    print(
        f"\nüíæ –°–æ—Ö—Ä–∞–Ω—è—é {len(apartments)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π –≤ —Ñ–∞–π–ª {full_filename}...")

    if file_format == "csv":
        try:
            # –î–ª—è CSV –Ω—É–∂–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤–ª–æ–∂–µ–Ω–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã (—Å–ø–∏—Å–∫–∏, —Å–ª–æ–≤–∞—Ä–∏)
            processed_apartments = []
            for ad in apartments:
                processed_ad = {}
                for key, value in ad.items():
                    if isinstance(value, (dict, list)):
                        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å–ª–æ–∂–Ω—ã–µ –ø–æ–ª—è –≤ —Å—Ç—Ä–æ–∫—É JSON, —á—Ç–æ–±—ã –æ–Ω–∏ –ø–æ–º–µ—Å—Ç–∏–ª–∏—Å—å –≤ –æ–¥–Ω—É —è—á–µ–π–∫—É
                        processed_ad[key] = json.dumps(
                            value, ensure_ascii=False)
                    else:
                        processed_ad[key] = value
                processed_apartments.append(processed_ad)

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–ª—é—á–∏ –ø–µ—Ä–≤–æ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ –æ–±—ä—è–≤–ª–µ–Ω–∏—è –∫–∞–∫ –∑–∞–≥–æ–ª–æ–≤–∫–∏
            headers = processed_apartments[0].keys()
            with open(full_filename, 'w', newline='', encoding='utf-8') as file:
                writer = csv.DictWriter(file, fieldnames=headers)
                writer.writeheader()
                writer.writerows(processed_apartments)
            print("‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ CSV.")

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤ CSV: {e}")

    elif file_format == "json":
        try:
            with open(full_filename, 'w', encoding='utf-8') as file:
                json.dump(apartments, file, ensure_ascii=False, indent=4)
            print("‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ JSON.")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤ JSON: {e}")
    else:
        print(
            f"‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {file_format}. –î–æ–ø—É—Å—Ç–∏–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã: 'csv', 'json'.")


if __name__ == "__main__":
    all_found_apartments = []
    current_url = BASE_URL
    pages_to_scrape = MAX_PAGES_TO_SCRAPE

    # ... (–≤–µ—Å—å –∫–æ–¥ –¥–ª—è —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –æ—Å—Ç–∞–µ—Ç—Å—è —Ç–∞–∫–∏–º –∂–µ, –∫–∞–∫ –≤ –ø—Ä–æ—à–ª—ã–π —Ä–∞–∑) ...
    # --- –®–ê–ì 1: –û–ø—Ä–µ–¥–µ–ª—è–µ–º, —Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–∞–Ω–∏—Ü –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å ---
    print("üöÄ –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞...")

    first_page_data = get_page_data(current_url)
    if not first_page_data:
        exit("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã.")

    if AUTO_DETECT_PAGES:
        total_ads = first_page_data.get('total_ads', 0)
        ads_per_page = len(first_page_data.get('apartments', [1]))
        if total_ads > 0 and ads_per_page > 0:
            pages_to_scrape = math.ceil(total_ads / ads_per_page)
            print(
                f"üìä –ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: –Ω–∞–π–¥–µ–Ω–æ {total_ads} –æ–±—ä—è–≤–ª–µ–Ω–∏–π. –¢—Ä–µ–±—É–µ—Ç—Å—è –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å ~{pages_to_scrape} —Å—Ç—Ä–∞–Ω–∏—Ü.")
        else:
            pages_to_scrape = 1
    else:
        print(
            f"‚öôÔ∏è –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω —Ä—É—á–Ω–æ–π —Ä–µ–∂–∏–º: –±—É–¥–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {pages_to_scrape} —Å—Ç—Ä–∞–Ω–∏—Ü.")

    all_found_apartments.extend(first_page_data['apartments'])
    print(
        f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(first_page_data['apartments'])} –æ–±—ä—è–≤–ª–µ–Ω–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ ‚Ññ1.")

    next_token = first_page_data['next_page_token']
    if not next_token:
        pages_to_scrape = 1

    # --- –®–ê–ì 2: –ó–∞–ø—É—Å–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü ---
    if pages_to_scrape > 1:
        for page_num in range(2, pages_to_scrape + 1):
            current_url = f"{BASE_URL}?cursor={next_token}"
            print(
                f"\n--- üìÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Å—Ç—Ä–∞–Ω–∏—Ü—É ‚Ññ{page_num} –∏–∑ {pages_to_scrape} ---")

            print(f"–ü–∞—É–∑–∞ {SLEEP_DELAY_SECONDS} —Å–µ–∫...")
            time.sleep(SLEEP_DELAY_SECONDS)

            page_data = get_page_data(current_url)

            if not page_data or not page_data['apartments']:
                print(
                    "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–ª–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –Ω–µ—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏–π. –ó–∞–≤–µ—Ä—à–∞—é.")
                break

            all_found_apartments.extend(page_data['apartments'])
            print(
                f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(page_data['apartments'])} –æ–±—ä—è–≤–ª–µ–Ω–∏–π. –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ: {len(all_found_apartments)}")

            next_token = page_data['next_page_token']
            if not next_token:
                print("\nüèÅ –≠—Ç–æ –±—ã–ª–∞ –ø–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞.")
                break

    print(
        f"\n\nüéâüéâüéâ –†–∞–±–æ—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ {len(all_found_apartments)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π.")

    # --- –®–ê–ì 3: –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã ---
    save_results(all_found_apartments, EXPORT_FORMAT, EXPORT_FILENAME)
