import math  # –î–æ–±–∞–≤–ª—è–µ–º –∏–º–ø–æ—Ä—Ç –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
import requests
import json
from bs4 import BeautifulSoup
import time
import math

# --- –ù–ê–°–¢–†–û–ô–ö–ò ---
# –ë–∞–∑–æ–≤—ã–π URL –¥–ª—è –ø–æ–∏—Å–∫–∞. –ü–æ–∑–∂–µ –º—ã –Ω–∞—É—á–∏–º—Å—è –¥–æ–±–∞–≤–ª—è—Ç—å —Å—é–¥–∞ —Ñ–∏–ª—å—Ç—Ä—ã.
BASE_URL = "https://re.kufar.by/l/minsk/snyat/kvartiru"
# –°–∫–æ–ª—å–∫–æ –º–∞–∫—Å–∏–º—É–º —Å—Ç—Ä–∞–Ω–∏—Ü –º—ã —Ö–æ—Ç–∏–º –æ–±–æ–π—Ç–∏
MAX_PAGES_TO_SCRAPE = 3
# –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏, —á—Ç–æ–±—ã –Ω–µ –Ω–∞–≥—Ä—É–∂–∞—Ç—å —Å–∞–π—Ç
SLEEP_DELAY_SECONDS = 2

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"
}


def get_page_data(url):
    """
    –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö (–æ–±—ä—è–≤–ª–µ–Ω–∏—è –∏ –ø–∞–≥–∏–Ω–∞—Ü–∏—è) —Å –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã.
    """
    try:
        response = requests.get(url, headers=HEADERS)
        response.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {url}: {e}")
        return None

    soup = BeautifulSoup(response.text, 'html.parser')
    script_tag = soup.find('script', id='__NEXT_DATA__')
    if not script_tag:
        return None

    try:
        data = json.loads(script_tag.string)
        listing_data = data['props']['initialState']['listing']

        regular_ads = listing_data.get('ads', [])
        vip_ads = listing_data.get('vip', [])
        all_apartments = vip_ads + regular_ads

        # –ù–∞—Ö–æ–¥–∏–º —Ç–æ–∫–µ–Ω –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        pagination_list = listing_data.get('pagination', [])
        next_page_token = None
        for page_info in pagination_list:
            if page_info.get('label') == 'next':
                next_page_token = page_info.get('token')
                break

        return {
            "apartments": all_apartments,
            "next_page_token": next_page_token
        }
    except (KeyError, json.JSONDecodeError) as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
        return None


# --- –ì–õ–ê–í–ù–ê–Ø –õ–û–ì–ò–ö–ê ---
if __name__ == "__main__":
    all_found_apartments = []
    current_url = BASE_URL

    # --- –®–ê–ì 1: –£–∑–Ω–∞–µ–º, —Å–∫–æ–ª—å–∫–æ –≤—Å–µ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü –Ω—É–∂–Ω–æ –æ–±–æ–π—Ç–∏ ---
    print(" sonde –°–Ω–∞—á–∞–ª–∞ –æ–ø—Ä–µ–¥–µ–ª–∏–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π...")
    first_page_data = get_page_data(current_url)

    if not first_page_data:
        exit("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É. –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü.")

    try:
        response = requests.get(current_url, headers=HEADERS)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        script_tag = soup.find('script', id='__NEXT_DATA__')
        full_data = json.loads(script_tag.string)

        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º total –≤ —á–∏—Å–ª–æ —Å –ø–æ–º–æ—â—å—é int()
        total_ads = int(full_data['props']
                        ['initialState']['listing'].get('total', 0))

        ads_on_first_page = len(first_page_data.get('apartments', []))

        if total_ads > 0 and ads_on_first_page > 0:
            max_pages = math.ceil(total_ads / ads_on_first_page)
            print(
                f"üìä –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ {total_ads} –æ–±—ä—è–≤–ª–µ–Ω–∏–π. –≠—Ç–æ –ø—Ä–∏–º–µ—Ä–Ω–æ {max_pages} —Å—Ç—Ä–∞–Ω–∏—Ü.")
        else:
            max_pages = 1

    except Exception as e:
        print(
            f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {e}. –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é –ª–∏–º–∏—Ç –≤ 1 —Å—Ç—Ä–∞–Ω–∏—Ü—É.")
        max_pages = 1

    all_found_apartments.extend(first_page_data['apartments'])
    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {ads_on_first_page} –æ–±—ä—è–≤–ª–µ–Ω–∏–π –Ω–∞ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ.")

    next_token = first_page_data['next_page_token']
    if next_token:
        current_url = f"{BASE_URL}?cursor={next_token}"
    else:
        max_pages = 1

    # --- –®–ê–ì 2: –ó–∞–ø—É—Å–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü ---
    if max_pages > 1:
        for page_num in range(2, max_pages + 1):
            print(
                f"\n--- üìÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Å—Ç—Ä–∞–Ω–∏—Ü—É ‚Ññ{page_num} –∏–∑ {max_pages} ---")

            print(f"–ü–∞—É–∑–∞ {SLEEP_DELAY_SECONDS} —Å–µ–∫...")
            time.sleep(SLEEP_DELAY_SECONDS)

            page_data = get_page_data(current_url)

            if not page_data or not page_data['apartments']:
                print(
                    "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–ª–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –Ω–µ—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏–π. –ó–∞–≤–µ—Ä—à–∞—é.")
                break

            all_found_apartments.extend(page_data['apartments'])
            print(
                f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(page_data['apartments'])} –æ–±—ä—è–≤–ª–µ–Ω–∏–π. –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ: {len(all_found_apartments)}")

            next_token = page_data['next_page_token']
            if next_token:
                current_url = f"{BASE_URL}?cursor={next_token}"
            else:
                print("\nüèÅ –≠—Ç–æ –±—ã–ª–∞ –ø–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞.")
                break

    print(
        f"\n\nüéâüéâüéâ –†–∞–±–æ—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ {len(all_found_apartments)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π.")
